{"hed":"Love and AI","story":[{"class":"reality","prose":[{"type":"text","value":"Omar and I matched 13 days before I left England. From that point on, we spent maybe 12 hours apart; the rest was <span class=active>walking hand in hand down the river Cam, watching the nonchalant punting tourists</span><span>eating dumplings on Jesus College lawn, getting lost in the library stacks</span><span>sitting in the grass on the lawn of King's College, waiting in line for the Footlights Pantomime</span>, filling in the outline of a love story I’d concocted in my head while reading novels and Instagram posts by women studying abroad at Cambridge, but mostly while getting dumped years prior by a different guy at university. It was the perfect end to what had been an otherwise unremarkable year."},{"type":"text","value":"Eventually I needed to leave for a new job in San Francisco. He needed to stay to finish his PhD. I begged the airline to let me change my flight. He sprinted to the train station to say goodbye."},{"type":"text","value":"I always assumed you don’t realize when you’re living in a cliche. But, in actuality, when you’re on one end of the romantic comedy trope, waiting at the station for a person who just so happens to look like they <span>could have appeared in an Ezra Pound poem</span>, you recognize and fear the inevitable final scene. Nothing can actually stay trite forever. In real life the ending is never so neat."}],"decision":[{"title":"I canceled my flight.","type":"generation","text":[{"type":"text","value":"But it was too late. He was gone."},{"type":"text","value":"I didn’t even know his last name."}]},{"title":"I got on the plane","type":"reality","text":[{"type":"text","value":"So, when he offered to move to be with me. I ended things. And days later when I changed my mind, he would not take me back, but agreed to try being friends."},{"type":"text","value":"I am not a writer, I am a machine learning engineer. In my work the question of whether something is cliche is steeped in data and trendlines: How close is it to the average of a distribution? How does it compare to the mode? Answering these questions requires poring over every piece of a dataset, extracting features that might mean something, identifying every possible inflection point. When I try to apply this energy to my love life (or lack thereof), friends tell me I’m “dwelling too much on the past” or “living in a fantasy world” or “not really their friend.”"},{"type":"text","value":"So, when two years had passed and my feelings for Omar still weren’t resolved, I didn’t tell my friends. I told a supercomputer."}]},{"title":"He offered to move.","type":"generation","text":[{"type":"text","value":"We spent the next week together, <br>planning a life in a city <br>where I had no job <br>and he had a year <br>and a half left of school. I volunteered at an NGO, <br>desperate to find a way to stay. <br>He took me to meet his family. <br>I took him to my parents’ house, <br>where he was treated <br>like a son."},{"type":"text","value":"We were in love <br>and in a state of permanent jet lag — <br>and we didn't care."},{"type":"text","value":"A year later, <br>we moved into a one-bedroom flat <br>in San Francisco <br>with the help of a dog named Cupcake. We lived in a world <br>of our own making, <br>where everyone was our friend <br>and nothing could touch us. We adopted a dog <br>named Cupcake. We were in love <br>and in a state of permanent jet lag — <br>and we didn't care."},{"type":"text","value":"It was magical."},{"type":"text","value":"Until it wasn't."}]}]},{"class":"gpt3","prose":[{"type":"text","value":"GPT-3 — the third “generative pre-trained transformer” released by the start-up (and my employer) OpenAI — is an example of a language model, or a tool that predicts what sequence of words should follow a user-provided prompt. For example, given a prompt like, “Hello my name” it will, more often than not, suggest that the next word is <span>“is.”</span>"},{"type":"text","value":"It happens to be the biggest publicly released model of its kind: 175 billion parameters. The next largest is 17 billion. You can think of a parameter roughly as a synapse; the human brain has around 100 trillion of those, but it has to focus on lots of things besides language, like swiping on dating apps and moving to San Francisco. To learn all of those parameters, GPT-3 is trained on hundreds of billions of sentences and stories from the internet and books. Written something on Reddit? There’s a good chance GPT-3 has read it."},{"type":"text","value":"This scale results in some remarkable generalizations: while the model has not eaten dumplings on the King’s College lawn, it has “read” Sylvia Plath and Caroline Calloway and every Modern Love essay ever written and it has “learned” heartbreak from Tosca and Taylor Swift and back issues of Sassy. Because the model has read from so many different sources with so many different authors, and also because it is a computer, the model lacks self-awareness, so, given a prompt like “Hi, I’m Pamela and Omar doesn’t love me” it will respond as me, Pamela, and <span>write the story of a relationship with all the poeticism and pathos and, yes, melodrama, that any young woman who has been dumped could ever want.</span>"},{"type":"text","value":"Often, GPT-3 reverts to calling me \"young woman\" or otherwise frames my writing as youthful and feminine — an excerpt from a teen magazine. I wonder who the young woman is. Was she also raised on Scorcese and Saturday Night Live? Will her writing eventually outgrow the young woman label? My mother points out I'm not really that young."},{"type":"text","value":"The model is far from a qualified therapist and access is governed by strict usage guidelines. Last August, I got access to the model for the first time. In the loneliness of month five of quarantine, having moved home to face my teenage journals, I didn’t know if I missed talking to strangers or if I missed talking to Omar. But I wanted to know if, with enough prodding, I could turn GPT-3 into either, or at least convince myself that I had."},{"type":"text","value":"I set out to get to know the model — could it write break-up songs, answer questions about a story, identify what went wrong in a relationship — as I tried to forget Omar."},{"type":"text","value":"I entered different permutations of my story, asking the model questions of what had gone wrong, hoping I could identify variables to land at the ending I wanted, or, I thought, find the variable that was to blame for the ending I had."}],"decision":[{"title":"His name was David instead.","type":"generation","text":[{"type":"text","value":"TEXT HERE"}]},{"title":"No variable mattered.","type":"reality","text":[{"type":"text","value":"One argument against the model’s intelligence is that it has a tendency to “overfit.” This happens when the model essentially memorizes the training data and assumes that everything else should look like it. You meet someone, you fall in love, you break up, and heartbroken you assume you'll never find anyone who compares. You look for a perfect substitute."},{"type":"text","value":"In the context of a language model, \"overfitting\" tends to show up as repetitive sequences of language. While models have gotten better at avoiding these traps as they’ve gotten larger, there are still sequences of words and language that will trigger this sort of behavior."}]},{"title":"I was older.","type":"generation","text":[{"type":"text","value":"TEXT HERE"}]}]},{"class":"reality","prose":[{"type":"text","value":"Over the first year after I left Cambridge, we oscillated between chatting multiple times a day and not speaking for weeks. When we did talk, we mostly discussed whomever we were seeing at the time. I was rarely seeing anyone so he did most of the talking. My friends would say not to text again."}],"decision":[{"title":"So I wouldn’t.","type":"generation","text":[{"type":"text","value":"TEXT HERE"}]},{"title":"But I would.","type":"reality","text":[{"type":"text","value":"And we would wind up back on the phone for a few hours."},{"type":"text","value":"It was casual enough that I was not surprised when he invited me to stay with him in London on a layover. There was anticipation but not expectation when we discussed how his roommates would feel about someone staying on the couch."},{"type":"text","value":"Around a week before I was due to arrive he stopped replying to me. Angry and embarrassed, I finally convinced him to meet me at the hotel my sister booked when she saw the writing on the walls. We walked and talked for eight hours and then we made out and then he said, “I finally feel comfortable being your friend.”"},{"type":"text","value":"My friends say to Venmo-request him for the price of the hotel room and the dress that blew no one away. They say to delete his number and texts and Facebook messages and not to forget the call history I might be able to use to get his number after I do all the other things. They say there’s no intelligence behind my desire to stay friends with him."},{"type":"text","value":"I use my call history to find his number, I text again. I don’t call for three months, he manages to ring at my most vulnerable. We all find it hard to avoid repetitive behavior even if we know it's wrong."}]},{"title":"So I deleted his number.","type":"generation","text":[{"type":"text","value":"TEXT HERE"}]}]},{"class":"gpt3","prose":[{"type":"text","value":"When you prompt GPT-3, you request a certain number of words back. A language model is only able to handle a limited number of words in prompt plus generation. This means that it may end your story mid-sentence or thought. Or, it may reach an ending and then keep generating, because it’s been instructed to talk for a certain amount of time about the characters it’s been prompted with. If there are two characters in its universe you are forced to engage with them, even after they and you should have moved on."},{"type":"text","value":"He called one day in the week preceding quarantine and I joked we should meet in New York. By then we were already used to conducting a friendship or relationship or something in between entirely on video chat. He smiled and said, “Actually, that would be pretty fun, I’ll look at flights, let’s talk soon.”"},{"type":"text","value":"The next day New York shut down. A couple weeks later he had met someone new on a dating app, “We’re just chatting though.” Two months passed and he called me."},{"type":"text","value":"<span class=active>“It’s funny, she’s in San Francisco and basically has your dream job.”</span><span>“Niiiice!”</span><span>“She visited last week and she’s moving to London in January.”</span><span>“So fun!”</span><span>“Happy to stay friends but know it’s probably weird for you.”</span><span>“Not at all,\" I said.</span>"},{"type":"text","value":"I was upset that he thought my dream job was to manage make-up artists. I was upset that he’d replaced me with someone who lived a few blocks away. I was upset that he probably told her he loved her at the train station. At least that’s what the model said happened."}]}]}