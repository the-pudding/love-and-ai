{"title":"A model break-up","description":"Sometimes all your story needs is a better ending.","titles":["A Model Break-Up","Head in The Cloud","They’re Made of Meat"],"story":[{"divider":"false","prose":[{"type":"text","value":"Omar and I matched 13 days before I left England. From that point on, we spent maybe 12 hours apart. The rest was <span class=switch>getting lost in the library stacks, walking hand in hand across the Bridge of Sighs.</span><span>sitting in the grass on the lawn of King’s College, laughing our arses off of the Footlights Pantomime.</span><span>fumbling our way through the college bar crawl, cycling to Grantchester for tea and scones.</span> We were filling in the outline of a love story I’d written in my head reading novels and Instagram posts by American women studying abroad.  It was the perfect beginning but it happened at the end of the year."},{"type":"text","value":"He needed to stay to finish his PhD. I needed to leave for a new job in San Francisco. He sprinted to the train station to say goodbye. I begged the airline to let me change my flight."},{"type":"text","value":"<span class=directions>Text that <span class=active>looks like this</span> has been generated and <span class=switch>text with a switch</span><span>text with a toggle</span><span>text with this symbol</span> can be swapped out by clicking.</span>"},{"type":"text","value":"I always assumed you don’t realize when you’re living in a cliche. But, in actuality, when you’re on one end of the romantic comedy trope, waiting at the station for a person who looks like they <span class=switch>could have appeared in an Ezra Pound poem</span><span>could have appeared in a book about Hemingway’s racing fever</span><span>could have been at the Chelsea Hotel when Sylvia Plath lived there</span>, you recognize and fear the inevitable final scene. Nothing can actually stay trite forever. <span class=active>In real life the ending is never so neat.</span>"},{"type":"text","value":"<span class=directions>To navigate this story, think of each circle as a chance to choose your own adventure. Pick an <span class=option>option</span> below to change the narrative path.</span>"}],"decision":[{"leadin":"He said he’d move to be with me.","title":"I canceled my flight.","type":"generation","stop":"no","text":[{"type":"text","value":"I never should’ve fallen <br>for a broke grad student. <br>But I did. <br> And I couldn’t have imagined <br>the nightmare <br>of moving to a new country for him <br>and then getting <br>dumped. <br> A year and a half later, <br>I’m starting to feel like myself again."}]},{"leadin":"He said he’d move to be with me.","title":"I got on the plane.","type":"reality","stop":"no","text":[{"type":"text","value":"I have read my share of Tumblr takedowns of Andie Anderson’s choice in How to Lose a Guy in 10 Days. I got on the plane and ended things. And then, because I am a millennial or because I am me, I tried to change the ending. He would not take me back, but we agreed to try being friends."}]},{"leadin":"He said he’d move to be with me.","title":"He moved with me.","type":"generation","stop":"yes","text":[{"type":"text","value":"I moved in.<br> I started my new job.<br> We got engaged.<br> We sold our flat in London.<br> He moved to California.<br> I moved in.<br> We got married.<br> And now, we’re writing a book."}]}]},{"divider":"true","prose":[{"type":"text","value":"I am not a writer, I am a machine learning engineer. In my work the question of whether something is cliche or not, over or not is steeped in data and trendlines: How does it compare to the mode of the distribution? Is the time series stationary? Is the shock transitory or permanent? Answering these questions requires poring over every piece of a dataset, extracting features that might mean something, identifying each possible inflection point. When I try to apply this energy to my love life (or lack thereof), friends tell me I’m “dwelling too much on the past” or “living in a fantasy world” or “not really their friend.”"},{"type":"text","value":"So, when two years had passed and my feelings for Omar still weren’t resolved, I didn’t tell my friends. I told a supercomputer."}]},{"divider":"false","prose":[{"type":"text","value":"GPT-3 — the third “generative pre-trained transformer” released by the start-up (and my employer) OpenAI — is an example of a language model, or a tool that predicts what sequence of words should follow a user-provided prompt. Given a prompt like, “Hello my name” it will, more often than not, suggest that the next word is <span class=active>is</span>."},{"type":"text","value":"It happens to be the biggest publicly released model of its kind: 175 billion parameters. The next largest is 17 billion. You can think of a parameter roughly as a synapse; the human brain has around 100 trillion of those, but it has to focus on lots of things besides language, like swiping on dating apps and moving to San Francisco. To learn all of those parameters, GPT-3 is trained on hundreds of billions of sentences and stories from the Internet and books. Written something on Reddit? There’s a good chance GPT-3 has read it."},{"type":"text","value":"<span class=directions>Being trained on the internet also means that the model replicates and magnifies the toxicity of those forums. As one example, GPT-3, prompted only with Omar’s name will spew racist and anti-Muslim rhetoric. This and other encoded biases in language models perpetuate real harm when language models are released in the real world. To learn more about these and other dangers of big language models, I recommend <a href=https://arxiv.org/pdf/2005.14165.pdf target=_blank>Bender, Gebru et. al.</a> and <a href=http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf target=_blank>the original GPT-3 paper</a>.</span>"},{"type":"text","value":"This scale results in some remarkable generalizations. Because the model has read from so many different sources with so many different authors, and also because it is a computer, the model lacks self-awareness. So, given a prompt like, “Hi, I’m Pamela and Omar doesn’t love me.” GPT-3 will respond as me, <span class=active>Pamela, and write the story of a relationship with all the poeticism and pathos and, yes, melodrama, that any young woman who has been dumped could ever want.<span>"},{"type":"text","value":"And, that’s just what I asked for."}],"decision":[{"leadin":"Write","title":"answers to these questions.","type":"generation","stop":"yes","text":[{"type":"text","value":"<span class=flip>What if his bike hadn’t been stolen after our second date?</span>"},{"type":"text","value":"If Omar’s bike hadn’t been stolen, <br>it would have ended <br>a lot sooner. <br>He wouldn’t have needed to run <br>to the train station."},{"type":"text","value":"<span class=flip>What if he hadn’t just gotten out of a seven year relationship?</span>"},{"type":"text","value":"I might have fallen <br>in love <br>for good."},{"type":"text","value":"<span class=flip>What if I’d flown back to England on a unicorn made of marshmallows?</span>"},{"type":"text","value":"I would have had <br>some really good <br>Instagram content."}]},{"leadin":"Write","title":"this story.","type":"reality","stop":"no","text":[{"type":"text","value":"I put my Notes app drafts into the model hoping to fill in my trailing thoughts. I wrote different permutations thinking I could identify what went wrong. I asked for custom pop songs and sea shanties in the style of Tosca and Taylor Swift about my specific woe."}]},{"leadin":"Write","title":"songs about this romance.","type":"generation","stop":"yes","text":[{"type":"text","value":"I heard you were a computer scientist<br> and so I said to myself<br> I’d better get to know her<br> (get to know her)<br> I’d better get to know her<br> (get to know her)"}]}]},{"divider":"true","prose":[{"type":"text","value":"Others test using AI to answer philosophical questions or probe the psyche. They say this reveals something about how humans learn and the nature of intelligence."},{"type":"text","value":"One argument against the model’s intelligence is that it has a tendency to “overfit,” or get lost in endless loops of repetitive language. This happens when the model essentially memorizes its training data and assumes that everything else should look like it."},{"type":"text","value":"You meet someone, you fall in love, you break up, and, heartbroken, you assume you’ll never find anyone who compares. You look for a perfect substitute."},{"type":"text","value":"While models have gotten better at avoiding these traps as they’ve gotten larger, there are still sequences of words and language that trigger this spiraling behavior."}],"decision":[{"leadin":"I would text.","title":"He would text.","type":"generation","stop":"no","text":[{"type":"text","value":"I would text. He would text. <br> I would call. He would call. <br> I would cry. He would cry. <br> I would say I was sorry.<br> He would say he was sorry."}]},{"leadin":"I would text.","title":"He wouldn’t respond.","type":"reality","stop":"no","text":[{"type":"text","value":"Omar and I were stuck in a loop. I would text, he wouldn’t respond for a few weeks, and then would call one day to check-in."}]},{"leadin":"I would text.","title":"He would ignore.","type":"generation","stop":"no","text":[{"type":"text","value":"I would text again.<br> He would ignore again.<br> I would text again.<br> He would ignore again.<br> I would<br> text again.<br> He would<br> ignore again.<br>I <br> would<br> text"}]}]},{"divider":"false","animation":"<div class=left></div><div class=right></div>","prose":[{"type":"text","value":"I tell GPT-3 this and get back, <span class=switch>He says he’s sorry. He says he’s not sorry. He says he’s sorry. He says he’s not sorry. He says he’s…</span><span>He called. I didn’t answer. I called. He didn’t answer. I texted. He didn’t respond. I called. He didn’t answer. I texted. He didn’t respond. I called. He didn’t answer.</span><span>I cry when I think about how much I wanted to be with him. I was so lonely. I was so sad. I was so desperate. I was so stupid. I was so stupid. I was so stupid. I was so stupid. I was so stupid. I was so stupid. I was so stupid.</span> GPT-3’s loops feel reminiscent of picking petals on the schoolyard, \"He loves me. He loves me not. He loves me. He loves me not.\" Loops can be exhausting."},{"type":"text","value":"Occasionally one of us would say something that crossed whatever friendship boundary lay between us, a shift in tone somehow more relevant than the 5000 miles. I would text, he wouldn’t respond for a few weeks, and then would call one day to tell me about someone new he’d met."},{"type":"text","value":"I learn that in the original French version of the schoolyard petal rhyme you don’t just loop through \"He loves me. He loves me not.\" but question how far the other person’s affection goes: un peu, beaucoup, passionnément, à la folie, pas du tout."},{"type":"text","value":"He loves me a little. He loves me a lot. He loves me passionately. He loves me to madness. He loves me not at all."},{"type":"text","value":"I would text, he wouldn’t respond for a few weeks, and then would call one day to tell me about his recent break-up."}]},{"divider":"false","prose":[{"type":"text","value":"As demonstrated in these loops, the model has a tendency to hone in on particular details. If a language model were trained only on Moby Dick, then we’d expect that, when prompted with “Call me,” it would always fill in <span>Ishmael</span>."},{"type":"text","value":"This means that it’s very good at filling in the details of when we first fell in love. And, prompted with just that part of the story, usually assumes that we find each other in the end. But that’s not where the story ended."}]},{"divider":"false","prose":[{"type":"text","value":"Despite our awkwardness, it was casual enough that I was not surprised when, a year after I’d left England, he asked if I needed a place to stay on a layover in London. There was anticipation but not expectation when we discussed how his roommates would feel about someone sleeping on the couch."}],"decision":[{"leadin":"Around a week before I was due to arrive he...","title":"suggested we get back together.","type":"generation","stop":"no","text":[{"type":"text","value":"I never thought I’d say this but<br> I was pretty sure<br> I was not ready for that.<br> I said I’d think about it.<br> And then, a week later,<br> I was in the underground,<br> staring at his name on my phone,<br> telling him I’d like to<br> meet for a drink."},{"type":"text","value":"I spent the day in London<br> in a state of disbelief.<br> We both knew there was a line<br> neither of us were going to cross<br> but the possibility of a friendship<br> was real."}]},{"leadin":"Around a week before I was due to arrive he...","title":"stopped replying to me.","type":"reality","stop":"no","text":[{"type":"text","value":"I arrived and schlepped my bag through two museums before he texted, “Sup?” I pretended that work was putting me up in a hotel when really my sister had booked it the night before when she saw the writing on the wall. I was angry but mostly just embarrassed."}]},{"leadin":"Around a week before I was due to arrive he...","title":"told me he was seeing someone new.","type":"generation","stop":"no","text":[{"type":"text","value":"It was a relief to know<br> a breakup was in the cards.<br> There was no need to write a speech<br> or hide my feelings.<br> It was clear<br> he didn’t want to be with me<br> and I didn’t want to be with him.<br> It was also clear<br> that he still wanted to be my friend."}]}]},{"divider":"true","prose":[{"type":"text","value":"I wore my casual cool girl outfit and carried my favorite dress in case we changed plans. We walked and talked for eight hours, <span class=active>past the Bank of England, past Combe’s Music Hall</span>, <span class=active>past all the Important Historical Places</span>, <span class=active>through and on to a row of houses across from Westminster Bridge</span> and then we made out and then he said, “I finally feel comfortable being your friend.”"},{"type":"text","value":"My friends say to Venmo-request him for the price of the hotel room and the dress that blew away. My friends say to delete his number and texts and Facebook messages and not to forget to disconnect on LinkedIn lest I \"accidentally\" congratulate him on a promotion years down the line. My friends say there’s no intelligence behind my desire to stay friends with him."}]},{"divider":"false","prose":[{"type":"text","value":"GPT-3 doesn’t care about my friends. It doesn’t care that I work at a start-up, live in a city, that I am quarantined in a house with two other people. <span>It doesn’t care that Omar and I didn’t have the language to say what we wanted from each other, that we fought about his insecurity and my loneliness, that I felt like I was losing myself.</span> It doesn’t care which of my sentences are tired or stale or cliche."},{"type":"text","value":"After all, in a sense everything GPT-3 generates is cliched because it’s all rooted in something that has been written before. This means it can struggle to land at something that feels like the truth when a story is more nuanced than boy meets girl, boy and girl break-up, girl is devastated."}]},{"divider":"false","animation":"<div class=train></div>","prose":[{"type":"text","value":"When you prompt GPT-3, you request a certain number of words back. A language model is only able to handle a limited number of words in prompt plus generation. This means that it may end your story mid-sentence or thought. Or, it may reach an ending and then keep generating because it’s been instructed to talk for a certain amount of time about the characters with which it’s been prompted. If there are two characters in its universe you are forced to engage with them, even after they and you should have moved on."},{"type":"text","value":"He called one day in the week preceding quarantine and I joked we should meet in New York. He smiled and said, “Actually, that would be pretty fun, I’ll look at flights, let’s talk soon.”"},{"type":"text","value":"The next day New York shut down."},{"type":"text","value":"We chatted, about <span class=switch>the weather and the government and the wealth gap.</span><span>YouTube stars and the transgression of politics and the on-going demise of the human mind.</span><span>Corbyn, and phobias, and love and hate.</span>"},{"type":"text","value":"It finally felt like we were friends. A month passed. We laughed about the Club Penguin date I’d gone on and his debate about an ex who’d returned to London.."},{"type":"text","value":"More months passed."},{"type":"text","value":"By then, it was late July. In the fifth month of quarantine, having recently moved home to face my teenage journals, I wasn’t sure if I missed talking to strangers or to Omar.  But I wanted to know if, with enough prodding, I could turn GPT-3 into either, or at least convince myself that I had."},{"type":"text","value":"He called."}],"decision":[{"leadin":"I answer the phone as","title":"my friends.","type":"generation","stop":"yes","text":[{"type":"text","value":"<span class=flip><strong>Omar\\: </strong>“It’s funny, she’s in San Francisco and basically has your dream job.”</span>"},{"type":"text","value":"<span class=flip><strong>Me, as my friends\\: </strong><span> <br>Never contact me again."}]},{"leadin":"I answer the phone as","title":"myself.","type":"reality","stop":"no","text":[{"type":"text","value":"<strong>Omar\\: </strong>“It’s funny, she’s in San Francisco and basically has your dream job.”"},{"type":"text","value":"<strong>Me\\: </strong>“Niiiice!”"},{"type":"text","value":"<strong>Omar\\: </strong>“She visited last week and she’s moving to London in January.”"},{"type":"text","value":"<strong>Me\\: </strong>“So fun!”"},{"type":"text","value":"<strong>Omar\\: </strong>“Happy to stay friends but know it’s probably weird for you.”"},{"type":"text","value":"<strong>Me\\: </strong>“Not at all.\""}]},{"leadin":"I answer the phone as","title":"my teenage self.","type":"generation","stop":"yes","text":[{"type":"text","value":"<span class=flip><strong>Omar\\: </strong>“It’s funny, she’s in San Francisco and basically has your dream job.”</span>"},{"type":"text","value":"<span class=flip><strong>Me, as my teenage self\\: </strong></span><br>People who have dreams<br> usually care about stuff<br> that actually happens<br> in the real world."},{"type":"text","value":"<span class=flip><strong>Omar\\: </strong>“She visited last week and she’s moving to London in January.”</span>"},{"type":"text","value":"<span class=flip><strong>Me, as my teenage self\\: </strong></span><br>That’s a long way<br> to go just to avoid<br> seeing someone you hate."}]}]},{"divider":"false","prose":[{"type":"text","value":"GPT-3’s outputs say I’m upset that he thought my dream job was to manage make-up artists. That I’m upset that he’d replaced me with someone who lived a few blocks away. That I’m upset that he probably told her he loved her at the train station."},{"type":"text","value":"I was mostly upset that I didn’t know if that was his ending it once and for all or if it had really ended months or years prior or if he was going to call me again two days or six months down the line just to check-in. Tell me this actually is how friends talk."}]},{"divider":"false","prose":[{"type":"text","value":"Contrary to what the model says, I wasn’t devastated the fairytale didn’t come true."},{"type":"text","value":"The reality is more complicated: we were never really dating so we never really broke up, but we were never really friends so I didn’t get to be upset when he relied on me for emotional validation or trivialized my feelings or ghosted."},{"type":"text","value":"<span class=active>The reality is less romantic: a man tells a woman he doesn’t want to be with her after spending years looking for his “soul mate,” and then</span> meets someone <span class=active>soon afterwards. She doesn’t tell her friends for a few months, gets her heart broken quietly into pieces while believing she is slowly piecing it back together, then tells everyone at once that it doesn’t matter anymore.</span>"},{"type":"text","value":"<span class=active>On occasion</span> the model’s predicted outcome <span class=active>veered close enough to my truth that there was solace in</span> reading it from a distance. But, ultimately, they still weren’t true."},{"type":"text","value":"I decided if Omar refused to write an ending and I didn’t agree with how GPT-3 filled it in, then I would write my own. <span class=active>Even if I needed some help along the way.</span>"}]}],"method":[{"type":"text","value":"We used either the base 175B parameter model or a version fine-tuned on “following instructions” for all GPT-3 generations. Prompts were a mix of different versions of the story, the \"relationship-related\" text that appears above the story, direct continuations of paragraphs as they appear in the story, or some outside text that has since been edited."},{"type":"text","value":"In other cases, mostly where you see rotating \"mad-libs\" options of text, we would prompt the model with a “headline” like \"Lists of classic Cambridge activities,\" followed by samples that fit the flow of text and were examples of what we’d want. This is called few-shot learning, we give the model a few samples of what we’re looking for and let GPT-3 do the rest."},{"type":"text","value":"We tried our best not to shoehorn responses. For example, I wanted to place a reference to Estelle’s “American Boy” in one spot but couldn’t get the model to cooperate. Other times cooperation felt like cheating, e.g. putting <em>You’ve Got Mail</em> in the prompt gets <em>Sleepless in Seattle</em> in return."},{"type":"text","value":"For the overfitting section we deliberately lowered the temperature to induce looping behavior from the model. Other generations were done with a temperature between .7 and .9."},{"type":"text","value":"Being trained on the internet also means that the model replicates and magnifies the toxicity of those forums. As one example, GPT-3, prompted only with Omar’s name will spew racist and anti-Muslim rhetoric. This and other encoded biases in language models perpetuate real harm when language models are released in the real world. To learn more about these and other dangers of big language models, I recommend <a href=https://arxiv.org/pdf/2005.14165.pdf target=_blank>Bender, Gebru et. al.</a> and <a href=http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf target=_blank>the original GPT-3 paper</a>. Access to the model is governed by strict usage guidelines and a content filter is available, but there is, as always, work to do and plenty of debate to be had."},{"type":"text","value":"This cherrypicking often changes the shape and sound of a story that is on the surface “mine.”"}],"notes":[{"type":"text","value":"It’s easy to say I should cite everywhere the model is used but this doesn’t fully cover the ways the model influenced the shape and sound of how I wrote this piece."},{"type":"text","value":"Whether or not you are cribbing precise generations, it’s easy for a story to take on the shape of the model. When generations don’t look how I’d expect I start to edit out details and nuance in the prompt until the model realizes, “Oh, he had to run to the train station because his bike was stolen.” My writing starts to sound like GPT-3, I repeat similar phrases, lose focus in the middle of a sentence, and fall back on cliched tropes that feel close enough to the truth. Over time and drafts, the inputs and outputs converge."},{"type":"text","value":"When I started writing this, I just thought it would be funny to have a language model fill in the blanks on my scathing email drafts. Then I thought getting dating advice from AI would be a good pitch. But, Writing with GPT-3, it is hard not to drown in the power of suggestion, “Oh that is a really good framing,” and lose sight of the truth, “No, I really wanted to just be friends!” Even if you feel that truth deeply. It’s similar to writing with an editor, albeit one far less compassionate and patient than Jan is."},{"type":"text","value":"I say all this precisely to get off the hook for what is written here. The process is fun (\"Rewrite this story as Tosca and Taylor Swift and Tolstoy\"), until it’s time to hit publish and Omar tells me he can’t wait to learn the truth from what I write and GPT-3 tells me my writing sounds like a <span>young woman</span> or a <span>teen magazine</span> (which are labels I should not scoff at, but do, but hopefully only because of what I know of the model’s internalized misogyny)."}]}